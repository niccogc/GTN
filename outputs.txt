
[FATAL] CUDA out of memory - terminating job: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.49 GiB of which 17.31 MiB is free. Process 2582424 has 1.23 GiB memory in use. Process 3021026 has 568.00 MiB memory in use. Including non-PyTorch memory, this process has 37.68 GiB memory in use. Of the allocated memory 34.83 GiB is allocated by PyTorch, and 2.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
======================================================================
EXPERIMENT PLAN SUMMARY
======================================================================
Total experiments: 4
Grid combinations: 4
Seeds per combination: 1

Parameter Grid:
  model: ['MPO2', 'MPO2TypeI', 'MMPO2', 'MMPO2TypeI'] (n=4)
  L: [4] (n=1)
  bond_dim: [12] (n=1)
  jitter_start: [1.0] (n=1)

Fixed Parameters:
  batch_size: 512
  n_epochs: 20
  jitter_decay: 0.5
  adaptive_jitter: False
  patience: 10
  min_delta: 1e-05
  train_selection: False
  init_strength: 0.01

Example runs (first 5):
  1. MPO2-L4-d12-jit1-seed42
  2. MPO2TypeI-L4-d12-jit1-seed42
  3. MMPO2-L4-d12-jit1-seed42
  4. MMPO2TypeI-L4-d12-jit1-seed42

Loading dataset: abalone...
  Dataset: abalone
  Train: 2923 samples
  Val: 627 samples
  Test: 627 samples
  Features: 11 (+1 bias = 11)
  Task: regression
  Device: cuda


[1/4] Running: MPO2-L4-d12-jit1-seed42
  Experiment log saved to: experiment_logs/debug_batch_MPO2_L4_d12_j1_s42.json
  ✓ Train: R²=0.6093 | Val: R²=0.5117

[2/4] Running: MPO2TypeI-L4-d12-jit1-seed42
  Experiment log saved to: experiment_logs/debug_batch_MPO2TypeI_L4_d12_j1_s42.json
  ✓ Train: R²=0.6051 | Val: R²=0.5134

[3/4] Running: MMPO2-L4-d12-jit1-seed42
