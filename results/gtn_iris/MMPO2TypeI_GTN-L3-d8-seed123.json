{
  "run_id": "MMPO2TypeI_GTN-L3-d8-seed123",
  "success": false,
  "singular": false,
  "error": "Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)",
  "traceback": "Traceback (most recent call last):\n  File \"/zhome/6b/e/212868/GTN/experiments/run_grid_search_gtn.py\", line 504, in main\n    result = run_single_experiment(\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/zhome/6b/e/212868/GTN/experiments/run_grid_search_gtn.py\", line 294, in run_single_experiment\n    output = gtn_model(batch_data)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/zhome/6b/e/212868/GTN/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/zhome/6b/e/212868/GTN/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/zhome/6b/e/212868/GTN/model/typeI/gtn_typeI.py\", line 342, in forward\n    y = model(x)\n        ^^^^^^^^\n  File \"/zhome/6b/e/212868/GTN/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/zhome/6b/e/212868/GTN/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/zhome/6b/e/212868/GTN/model/base/GTN.py\", line 61, in forward\n    out = tn.contract(output_inds=[\"s\"] + self.output_dims, optimize=\"auto-hq\")\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/zhome/6b/e/212868/GTN/.venv/lib/python3.12/site-packages/quimb/tensor/tensor_core.py\", line 9354, in contract\n    return tensor_contract(\n           ^^^^^^^^^^^^^^^^\n  File \"/zhome/6b/e/212868/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/functools.py\", line 912, in wrapper\n    return dispatch(args[0].__class__)(*args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/zhome/6b/e/212868/GTN/.venv/lib/python3.12/site-packages/quimb/tensor/tensor_core.py\", line 310, in tensor_contract\n    data_out = array_contract(\n               ^^^^^^^^^^^^^^^\n  File \"/zhome/6b/e/212868/GTN/.venv/lib/python3.12/site-packages/quimb/tensor/contraction.py\", line 285, in array_contract\n    return ctg.array_contract(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/zhome/6b/e/212868/GTN/.venv/lib/python3.12/site-packages/cotengra/interface.py\", line 864, in array_contract\n    return expr(*arrays, backend=backend)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/zhome/6b/e/212868/GTN/.venv/lib/python3.12/site-packages/cotengra/contract.py\", line 784, in __call__\n    p_array = _tensordot(l_array, r_array, arg)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/zhome/6b/e/212868/GTN/.venv/lib/python3.12/site-packages/autoray/autoray.py\", line 2388, in numpy_like\n    return fn(a, b, dims=axes)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/zhome/6b/e/212868/GTN/.venv/lib/python3.12/site-packages/torch/functional.py\", line 1333, in tensordot\n    return _VF.tensordot(a, b, dims_a, dims_b)  # type: ignore[attr-defined]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)\n"
}