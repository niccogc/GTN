
Loading dataset: breast
  Task: classification
  Train: 398 samples
  Val: 85 samples
  Test: 86 samples
  Input dim: 30
  Output dim: 2
  Device: cuda
======================================================================
EXPERIMENT PLAN SUMMARY
======================================================================
Total experiments: 360
Grid combinations: 72
Seeds per combination: 5

Parameter Grid:
  model: ['MPO2', 'MPO2TypeI_GTN', 'MMPO2', 'MMPO2TypeI_GTN'] (n=4)
  L: [3, 4] (n=2)
  bond_dim: [8, 12, 16] (n=3)
  lr: [0.01, 0.001, 0.0001] (n=3)

Fixed Parameters:
  output_site: 1
  batch_size: 64
  n_epochs: 1000
  patience: 40
  min_delta: 1e-08
  optimizer: adamw
  init_strength: 0.01
  rank: 5

Example runs (first 5):
  1. MPO2-L3-d8-seed42
  2. MPO2-L3-d8-seed7
  3. MPO2-L3-d8-seed123
  4. MPO2-L3-d8-seed256
  5. MPO2-L3-d8-seed999
  ... (355 more)

[271/360] MMPO2TypeI_GTN-L3-d8-seed42 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[271/360] Running: MMPO2TypeI_GTN-L3-d8-seed42
  AIM run started: 1d263ec3f2b14653b0c44eb4
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[272/360] MMPO2TypeI_GTN-L3-d8-seed7 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[272/360] Running: MMPO2TypeI_GTN-L3-d8-seed7
  AIM run started: d66a2d330528424a9b544d93
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[273/360] MMPO2TypeI_GTN-L3-d8-seed123 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[273/360] Running: MMPO2TypeI_GTN-L3-d8-seed123
  AIM run started: a7a4e05b13db41f6b4d8d52b
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[274/360] MMPO2TypeI_GTN-L3-d8-seed256 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[274/360] Running: MMPO2TypeI_GTN-L3-d8-seed256
  AIM run started: 87ad2196ec8448079980ff44
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[275/360] MMPO2TypeI_GTN-L3-d8-seed999 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[275/360] Running: MMPO2TypeI_GTN-L3-d8-seed999
  AIM run started: 19f4e6ed3d4248a7a1047165
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[276/360] MMPO2TypeI_GTN-L3-d8-seed42 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[276/360] Running: MMPO2TypeI_GTN-L3-d8-seed42
  AIM run started: ea7e9051df6b4d68959a816c
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[277/360] MMPO2TypeI_GTN-L3-d8-seed7 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[277/360] Running: MMPO2TypeI_GTN-L3-d8-seed7
  AIM run started: 4f1823e6ec22490d8b338d0f
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[278/360] MMPO2TypeI_GTN-L3-d8-seed123 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[278/360] Running: MMPO2TypeI_GTN-L3-d8-seed123
  AIM run started: 1a73bf3364954fc482108939
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[279/360] MMPO2TypeI_GTN-L3-d8-seed256 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[279/360] Running: MMPO2TypeI_GTN-L3-d8-seed256
  AIM run started: 27637cfc3aab436fb6555aad
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[280/360] MMPO2TypeI_GTN-L3-d8-seed999 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[280/360] Running: MMPO2TypeI_GTN-L3-d8-seed999
  AIM run started: 22450a36ae6447098fef5648
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[281/360] MMPO2TypeI_GTN-L3-d8-seed42 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[281/360] Running: MMPO2TypeI_GTN-L3-d8-seed42
  AIM run started: 726321bb0b9d4b13a0fb94e0
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[282/360] MMPO2TypeI_GTN-L3-d8-seed7 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[282/360] Running: MMPO2TypeI_GTN-L3-d8-seed7
  AIM run started: 9f084d43dbea4056920a3cbc
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[283/360] MMPO2TypeI_GTN-L3-d8-seed123 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[283/360] Running: MMPO2TypeI_GTN-L3-d8-seed123
  AIM run started: 3f2db6bc320a4b328394e7b9
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[284/360] MMPO2TypeI_GTN-L3-d8-seed256 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[284/360] Running: MMPO2TypeI_GTN-L3-d8-seed256
  AIM run started: 7d93df310e8d44439094dee4
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[285/360] MMPO2TypeI_GTN-L3-d8-seed999 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[285/360] Running: MMPO2TypeI_GTN-L3-d8-seed999
  AIM run started: f180c35a3ab84693b4f29c0a
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[286/360] MMPO2TypeI_GTN-L3-d12-seed42 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[286/360] Running: MMPO2TypeI_GTN-L3-d12-seed42
  AIM run started: bc36504220b24375a6ac44e7
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[287/360] MMPO2TypeI_GTN-L3-d12-seed7 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[287/360] Running: MMPO2TypeI_GTN-L3-d12-seed7
  AIM run started: 8f28a7cba5d54723a094bb88
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[288/360] MMPO2TypeI_GTN-L3-d12-seed123 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[288/360] Running: MMPO2TypeI_GTN-L3-d12-seed123
  AIM run started: 45eb9fefb0df42b0babe8988
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[289/360] MMPO2TypeI_GTN-L3-d12-seed256 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[289/360] Running: MMPO2TypeI_GTN-L3-d12-seed256
  AIM run started: 321b33da2c334af5b01115d8
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[290/360] MMPO2TypeI_GTN-L3-d12-seed999 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[290/360] Running: MMPO2TypeI_GTN-L3-d12-seed999
  AIM run started: 32821be5aaa040fba586471f
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[291/360] MMPO2TypeI_GTN-L3-d12-seed42 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[291/360] Running: MMPO2TypeI_GTN-L3-d12-seed42
  AIM run started: b4cfc1330f104351921e7eb5
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[292/360] MMPO2TypeI_GTN-L3-d12-seed7 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[292/360] Running: MMPO2TypeI_GTN-L3-d12-seed7
  AIM run started: 976bd61695874f859be950f5
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[293/360] MMPO2TypeI_GTN-L3-d12-seed123 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[293/360] Running: MMPO2TypeI_GTN-L3-d12-seed123
  AIM run started: ce095c9e50074a4c8776c893
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[294/360] MMPO2TypeI_GTN-L3-d12-seed256 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[294/360] Running: MMPO2TypeI_GTN-L3-d12-seed256
  AIM run started: ec5da14757214188abe44f70
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[295/360] MMPO2TypeI_GTN-L3-d12-seed999 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[295/360] Running: MMPO2TypeI_GTN-L3-d12-seed999
  AIM run started: 03f1cbef97c44624ba885599
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[296/360] MMPO2TypeI_GTN-L3-d12-seed42 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[296/360] Running: MMPO2TypeI_GTN-L3-d12-seed42
  AIM run started: 124228c05b7f42759f5719a7
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[297/360] MMPO2TypeI_GTN-L3-d12-seed7 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[297/360] Running: MMPO2TypeI_GTN-L3-d12-seed7
  AIM run started: f648004bf48c47ce854598c3
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[298/360] MMPO2TypeI_GTN-L3-d12-seed123 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[298/360] Running: MMPO2TypeI_GTN-L3-d12-seed123
  AIM run started: 0fd350e865654d3990b03f62
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[299/360] MMPO2TypeI_GTN-L3-d12-seed256 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[299/360] Running: MMPO2TypeI_GTN-L3-d12-seed256
  AIM run started: 08a150e89ef74511a61eeee8
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[300/360] MMPO2TypeI_GTN-L3-d12-seed999 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[300/360] Running: MMPO2TypeI_GTN-L3-d12-seed999
  AIM run started: 6867955011d2426287b7231a
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[301/360] MMPO2TypeI_GTN-L3-d16-seed42 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[301/360] Running: MMPO2TypeI_GTN-L3-d16-seed42
  AIM run started: 4ecc899d45f64f4682bcbedb
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[302/360] MMPO2TypeI_GTN-L3-d16-seed7 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[302/360] Running: MMPO2TypeI_GTN-L3-d16-seed7
  AIM run started: 6749ed1b56a2424e988ed914
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[303/360] MMPO2TypeI_GTN-L3-d16-seed123 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[303/360] Running: MMPO2TypeI_GTN-L3-d16-seed123
  AIM run started: 1c3b9ff0844e461d846a74a5
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[304/360] MMPO2TypeI_GTN-L3-d16-seed256 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[304/360] Running: MMPO2TypeI_GTN-L3-d16-seed256
  AIM run started: 69d425cfdda34642a2384096
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[305/360] MMPO2TypeI_GTN-L3-d16-seed999 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[305/360] Running: MMPO2TypeI_GTN-L3-d16-seed999
  AIM run started: 13d160db62264dababf74b5b
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[306/360] MMPO2TypeI_GTN-L3-d16-seed42 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[306/360] Running: MMPO2TypeI_GTN-L3-d16-seed42
  AIM run started: 14b876b12e29448fab2b8d67
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[307/360] MMPO2TypeI_GTN-L3-d16-seed7 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[307/360] Running: MMPO2TypeI_GTN-L3-d16-seed7
  AIM run started: a2e616b53ef349fc85785282
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[308/360] MMPO2TypeI_GTN-L3-d16-seed123 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[308/360] Running: MMPO2TypeI_GTN-L3-d16-seed123
  AIM run started: 6991f9fa3f4d4b41a3b324c1
  ✗ FAILED: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_mm)
[309/360] MMPO2TypeI_GTN-L3-d16-seed256 - RETRYING (Expected all tensors to be on the same device, but got mat2 is on cuda:0, differ...)

[309/360] Running: MMPO2TypeI_GTN-L3-d16-seed256
  AIM error (attempt 1/3): Expecting value: line 1 column 1 (char 0)
  Retrying in 10s...
  AIM error (attempt 2/3): Expecting value: line 1 column 1 (char 0)
  Retrying in 10s...

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 27685593: <gtn-breast> in cluster <dcc> Exited

Job <gtn-breast> was submitted from host <hpclogin1> by user <nicci> in cluster <dcc> at Sun Jan 25 14:07:46 2026
Job was executed on host(s) <8*n-62-20-11>, in queue <gpuv100>, as user <nicci> in cluster <dcc> at Sun Jan 25 15:35:30 2026
</zhome/6b/e/212868> was used as the home directory.
</zhome/6b/e/212868/GTN> was used as the working directory.
Started at Sun Jan 25 15:35:30 2026
Terminated at Sun Jan 25 15:46:29 2026
Results reported at Sun Jan 25 15:46:29 2026

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/sh
#BSUB -q gpuv100
#BSUB -J gtn-breast
#BSUB -W 3:00
#BSUB -n 8
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -R "rusage[mem=8GB]"
#BSUB -R "span[hosts=1]"
#BSUB -o logs/gtn-breast_%J.out
#BSUB -e logs/gtn-breast_%J.err
#BSUB -u nicci@dtu.dk

export HOME=/zhome/6b/e/212868

cd $HOME/GTN
source .venv/bin/activate

set -a
source $HOME/aim
set +a

python experiments/run_grid_search_gtn.py --config experiments/configs/uci_gtn_breast.json --output-dir results/gtn_breast

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   42.00 sec.
    Max Memory :                                 556 MB
    Average Memory :                             537.14 MB
    Total Requested Memory :                     65536.00 MB
    Delta Memory :                               64980.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                16
    Run time :                                   667 sec.
    Turnaround time :                            5923 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/gtn-breast_27685593.err> for stderr output of this job.

